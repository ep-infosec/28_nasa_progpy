{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample building a custom model with LSTMStateTransitionModel.\n\nFor most cases, you will be able to use the standard LSTMStateTransitionModel.from_data class with configuration (see the LSTMStateTransitionModel class for more details). However, sometimes you might want to add custom layers, or other complex components. In that case, you will build a custom model and pass it into LSTMStateTransitionModel.\n\nIn this example, we generate fake data using the BatteryElectroChemEOD model. This is a case where we're generating a surrogate model from the physics-based model. For cases where you're generating a model from data (e.g., collected from a testbed or a real-world environment), you'll replace that generated data with your own. \n\nWe build and fit a custom model using keras.layers. Finally, we compare performance to the standard format and the original model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom prog_models.data_models import LSTMStateTransitionModel\nfrom prog_models.models import BatteryElectroChemEOD\n\ndef run_example():\n    print('Generating data...')\n    batt = BatteryElectroChemEOD()\n    future_loading_eqns = [lambda t, x=None: batt.InputContainer({'i': 1+1.5*load}) for load in range(6)]\n    # Generate data with different loading and step sizes\n    # Adding the step size as an element of the output\n    training_data = []\n    input_data = []\n    output_data = []\n    for i in range(9):\n        dt = i/3+0.25\n        for loading_eqn in future_loading_eqns:\n            d = batt.simulate_to_threshold(loading_eqn, save_freq=dt, dt=dt) \n            u = np.array([np.hstack((u_i.matrix[:][0].T, [dt])) for u_i in d.inputs], dtype=float)\n            z = d.outputs\n            training_data.append((u, z))\n            input_data.append(u)\n            output_data.append(z)\n\n    # Step 2: Build standard model\n    print(\"Building standard model...\")\n    m_batt = LSTMStateTransitionModel.from_data(\n        inputs = input_data,\n        outputs = output_data,  \n        window=12, \n        epochs=30, \n        units=64,  # Additional units given the increased complexity of the system\n        input_keys = ['i', 'dt'],\n        output_keys = ['t', 'v'])   \n\n    # Step 3: Build custom model\n    print('Building custom model...')\n    (u_all, z_all) = LSTMStateTransitionModel.pre_process_data(training_data, window=12)\n    \n    # Normalize\n    n_inputs = len(training_data[0][0][0])\n    u_mean = np.mean(u_all[:,0,:n_inputs], axis=0)\n    u_std = np.std(u_all[:,0,:n_inputs], axis=0)\n    # If there's no variation- dont normalize \n    u_std[u_std == 0] = 1\n    z_mean = np.mean(z_all, axis=0)\n    z_std = np.std(z_all, axis=0)\n    # If there's no variation- dont normalize \n    z_std[z_std == 0] = 1\n\n    # Add output (since z_t-1 is last input)\n    u_mean = np.hstack((u_mean, z_mean))\n    u_std = np.hstack((u_std, z_std))\n\n    u_all = (u_all - u_mean)/u_std\n    z_all = (z_all - z_mean)/z_std\n\n    # u_mean and u_std act on the column vector form (from inputcontainer)\n    # so we need to transpose them to a column vector\n    normalization = (u_mean[np.newaxis].T, u_std[np.newaxis].T, z_mean, z_std)\n\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\"jena_sense.keras\", save_best_only=True)\n    ]\n    inputs = keras.Input(shape=u_all.shape[1:])\n    x = layers.Bidirectional(layers.LSTM(128))(inputs)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(z_all.shape[1] if z_all.ndim == 2 else 1)(x)\n    model = keras.Model(inputs, x)\n    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n    model.fit(u_all, z_all, epochs=30, callbacks = callbacks, validation_split = 0.1)\n\n    # Step 4: Build LSTMStateTransitionModel\n    m_custom = LSTMStateTransitionModel(model, \n        normalization=normalization, \n        input_keys = ['i', 'dt'],\n        output_keys = ['t', 'v']\n    )\n\n    # Step 5: Simulate\n    print('Simulating...')\n    t_counter = 0\n    x_counter = batt.initialize()\n    def future_loading(t, x=None):\n        return batt.InputContainer({'i': 3})\n\n    def future_loading2(t, x = None):\n        nonlocal t_counter, x_counter\n        z = batt.output(x_counter)\n        z = m_batt.InputContainer({'i': 3, 't_t-1': z['t'], 'v_t-1': z['v'], 'dt': t - t_counter})\n        x_counter = batt.next_state(x_counter, future_loading(t), t - t_counter)\n        t_counter = t\n        return z\n    data = batt.simulate_to_threshold(future_loading, dt=1, save_freq=1)\n    results = m_batt.simulate_to(data.times[-1], future_loading2, dt=1, save_freq=1)\n    results_custom = m_custom.simulate_to(data.times[-1], future_loading2, dt=1, save_freq=1)\n\n    # Step 6: Compare performance\n    print('Comparing performance...')\n    data.outputs.plot(title='original model', compact=False)\n    results.outputs.plot(title='generated model', compact=False)\n    results_custom.outputs.plot(title='custom model', compact=False)\n    plt.show()\n\nif __name__ == '__main__':\n    run_example()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}